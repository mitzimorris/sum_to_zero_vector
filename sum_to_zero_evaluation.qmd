---
title: The Sum-to-Zero Constraint in Stan
jupyter: python3
---

## Introducting the `sum_to_zero_vector` Constrained Parameter Type


As of Stan 2.36, there is a built in `sum_to_zero_vector` consrained parameter type:

```stan
parameters {
  sum_to_zero_vector[K] beta;
  // ...
}
```

This produces a vector of size `K` such that `sum(beta) = 0`.  In the
unconstrained representation requires only `K - 1` values because the
last is determined by the first `K - 1`.

Prior to Stan 2.36, a sum-to-zero constraint could be implemented in one of two ways:

- using a "hard" sum to zero constraint, where the parameter is declared to be an $N-1$ length vector with a corresponding $N$-length transformed parameter
whose first $N-1$ elements are the same as the corresponding parameter vector, and the $N^{th}$ element is the negative sum of the $N-1$ elements.
- using a "soft" sum to zero constraint with an $N$-length parameter vector whose sum is constrained to be within $\epsilon$ of $0$.

The performance of these implementations depends on the size of the parameter vector:
for small sizes, the hard sum-to-zero constraint is more efficient; for larger sizes, the soft sum-to-zero constraint is faster.

In this notebook we show how the `sum_to_zero_vector` constraint provides consistently better performance than the alternative ways of imposing a sum to zero constraint on a parameter vector by considering two different types of models:

- a multi-level regression for binomial data with group-level categorical predictors
- spatial models for areal data

```{python}
# libraries used in this notebook
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import plotnine as p9
import warnings
from libpysal.weights import Rook
from random import randint

from cmdstanpy import CmdStanModel, write_stan_json, cmdstan_path, cmdstan_version, rebuild_cmdstan

from utils_dataviz import *
from utils_nyc_map import nyc_sort_by_comp_size

import matplotlib
%matplotlib inline

warnings.filterwarnings('ignore')
```

```{python}
# notebook display options
np.set_printoptions(precision=2)
np.set_printoptions(suppress=True)
pd.set_option('display.precision', 2)
pd.options.display.float_format = '{:.2f}'.format

# setup plotnine look and feel
p9.theme_set(
  p9.theme_grey() + 
  p9.theme(text=p9.element_text(size=10),
        plot_title=p9.element_text(size=14),
        axis_title_x=p9.element_text(size=12),
        axis_title_y=p9.element_text(size=12),
        axis_text_x=p9.element_text(size=8),
        axis_text_y=p9.element_text(size=8)
       )
)
xlabels_90 = p9.theme(axis_text_x = p9.element_text(angle=90, hjust=1))
```

## Multi-level models with group-level categorical predictors

In this section we consider a model which estimates per-demographic rates of disease prevalence for a population.

The data is a set of per-demographic aggregated outcomes of a diagnostic test procedure with a specified sensitivity and specificity, and the demographic descriptor which is a vector of categorical values.
For verisimilitude, these are named `sex`, `age`, `eth`, and `edu`, and for each category, there are unequal numbers of observations.
In order to fit this model, we need to put a sum-to-zero constraint on the categorical variables.


### The data generating model

We have written a data-generating program to create datasets given the
baseline disease prevalence, test specificity and sensitivity,
the specified total number of diagnostic tests,
and number of categories for `age`, `eth`, and `edu`, `N_age`, `N_eth`, and `N_edu`, respectively.
The total number of observations is $2$ * `N_age` * `N_eth` * `N_edu`.

```stan
data {
  int<lower=1> N;  // total number of tests
  int<lower=1> N_age;
  int<lower=1> N_eth;
  int<lower=1> N_edu;
  real baseline;
  real<lower=0, upper=1> sens;
  real<lower=0, upper=1> spec;
}
transformed data {
  int strata = 2 * N_age * N_eth * N_edu;

}
```

In the generated quantities block we first generate:

* the true weights for the categorical coefficient vectors
* the distribution of per-category observations

```stan
  real beta_0 = baseline;
  // some difference by sex, unequal observations
  real beta_sex = normal_rng(0, 0.5);
  vector[2] pct_sex = [0.4, 0.6]';

  vector[N_age] pct_age = dirichlet_rng(rep_vector(2, N_age));
  vector[N_age] beta_age;
  for (n in 1:N_age) {
    beta_age[n] = std_normal_rng();
  }
  ...
```

Then we use a set of nested loops to generate the table of positivite tests, total tests per category.

```stan
  // compute aggregation by pcts
  array[strata] int sex;
  array[strata] int age;
  array[strata] int eth;
  array[strata] int edu;
  array[strata] int pos_tests;
  array[strata] int tests;
  array[strata] real p;
  array[strata] real p_sample;

  int idx = 1;
  for (i_sex in 1:2) {
    for (i_age in 1:N_age) {
      for (i_eth in 1:N_eth) {
        for (i_edu in 1:N_edu) {
          sex[idx] = i_sex;
          age[idx] = i_age;
          eth[idx] = i_eth;
          edu[idx] = i_edu;
          tests[idx] = to_int(pct_sex[i_sex] * pct_age[i_age] * pct_eth[i_eth] * pct_edu[i_edu] * N);
          p[idx] = inv_logit(beta_0 + beta_sex * (i_sex)
                    + beta_age[i_age] + beta_eth[i_eth] +  beta_edu[i_edu]);
          p_sample[idx] = p[idx] * sens + (1 - p[idx]) * (1 - spec);
          pos_tests[idx] = binomial_rng(tests[idx], p_sample[idx]);
          idx += 1;
        }
      }
    }
  }
```

With this program, we can observe how the different implementations perform under different amounts of data.


* Instantiate the data generating model.

```{python}
gen_mod = CmdStanModel(stan_file=os.path.join('stan', 
                                              'gen_binomial_4_preds.stan'))
```

* Choose total number of observations and number of categories for age, eth, and edu.

```{python}
gen_data_dict = {'N':90_000,
                 'N_age':10,
                 'N_eth':3,
                 'N_edu':8,
                'baseline': -3.9,
                'sens': 0.75,
                'spec': 0.9995}
print("total strata",
      (2 * gen_data_dict['N_age'] * gen_data_dict['N_eth'] * gen_data_dict['N_edu']))
```

* Run 1 sampling iteration to get a complete dataset.

```{python}
sim_data = gen_mod.sample(data=gen_data_dict,
                          iter_warmup=1, iter_sampling=1, chains=1)
```

* Examine the set of generated data-generating params and resulting dataset.

```{python}
for var, value in sim_data.stan_variables().items():
    if isinstance(value, (int, float)):
        print(var, value[0])
    else:
	print(var, value[0].head(10))
```

```{python}
sim_df = pd.DataFrame({'tests':sim_data.tests[0], 'pos_tests':sim_data.pos_tests[0], 'p_sample':sim_data.p_sample[0] })
sim_df['raw_prev'] = sim_df['pos_tests'] / sim_df['tests']
(
    p9.ggplot(sim_df)
    + p9.geom_density(p9.aes(x='raw_prev'), color='darkorange', fill='lightorange', alpha=0.5)
    + p9.geom_density(p9.aes(x='p_sample'), color='darkblue', fill='lightblue', alpha=0.5)
    + p9.labs(
        x='raw prevalance',
        y='',
        title='raw (orange) and true (blue) prevalence given test sensitivity, specificity across demographics'
    )
    + p9.theme_minimal()
)
```

### Model Fitting

Assemble the data dictionary of all input data for the model which solves the inverse problem - i.e., estimates regression coefficients given the observed data.  We use the generated data as the inputs.  Because the output files are real-valued outputs, regardless of variable element type, model data variables of type int need to be cast to int.  Here all the observed data is count and categorial data.

```{python}
data_4_preds = {'N':sim_data.pos_tests.shape[1], 
                'N_age':gen_data_dict['N_age'], 
                'N_eth':gen_data_dict['N_eth'],
                'N_edu':gen_data_dict['N_edu'],
                'pos_tests':sim_data.pos_tests[0].astype(int),
                'tests':sim_data.tests[0].astype(int),
                'sex':sim_data.sex[0].astype(int),
                'age':sim_data.age[0].astype(int), 
                'eth':sim_data.eth[0].astype(int),
                'edu':sim_data.edu[0].astype(int),
                'sens': gen_data_dict['sens'],
                'spec': gen_data_dict['spec'],
                'intercept_prior_mean': gen_data_dict['baseline'],
                'intercept_prior_scale': 2.5}

# print(data_4_preds)
```

* Capture the data-generating params

```{python}
true_params = {
    'beta_0': sim_data.beta_0[0],
    'pct_sex': sim_data.pct_sex[0],
    'beta_sex': sim_data.beta_sex[0],
    'pct_age': sim_data.pct_age[0],
    'beta_age':sim_data.beta_age[0],
    'pct_eth': sim_data.pct_eth[0],
    'beta_eth':sim_data.beta_eth[0],
    'pct_edu': sim_data.pct_edu[0],
    'beta_edu':sim_data.beta_edu[0]
}
true_params
```

#### Model 1: `sum_to_zero_vector`

```{python}
binomial_ilr_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_ilr.stan'))
```

```{python}
binomial_ilr_fit = binomial_ilr_mod.sample(data=data_4_preds, parallel_chains=4)
```

#### Model 2:  Hard sum-to-zero constraint

Run the sampler to get posterior estimates of the model conditioned on the data. 

```{python}
binomial_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_hard.stan'))
```

```{python}
binomial_hard_fit = binomial_hard_mod.sample(data=data_4_preds, parallel_chains=4)
```

#### Model 3:  soft sum-to-zero constraint

```{python}
binomial_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_soft.stan'))
```

```{python}
binomial_soft_fit = binomial_soft_mod.sample(data=data_4_preds, parallel_chains=4)
```

### Model Checking

Run CmdStan`s `diagnose` method to check model fits.

```{python}
print(binomial_ilr_fit.diagnose())
print(binomial_hard_fit.diagnose())
print(binomial_soft_fit.diagnose())
```

**Calibration check**
All models contian a `generated quantities` block, which creates `y_rep`,
the [posterior predictive sample](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html).
If the model is well-calibrated for the data, 
we expect that at least 50% of the time the observed value of `y` will fall in the central 50% interval of the `y_rep` sample estimates.


```{python}
y_rep_ilr = binomial_ilr_fit.y_rep.astype(int)
print("ILR fit", ppc_central_interval(y_rep_ilr, sim_data.pos_tests[0])

y_rep_hard = binomial_hard_fit.y_rep.astype(int)
print("Hard sum-to-zero fit", ppc_central_interval(y_rep_hard, sim_data.pos_tests[0])

y_rep_soft = binomial_soft_fit.y_rep.astype(int)
print("Soft sum-to-zero fit", ppc_central_interval(y_rep_soft, sim_data.pos_tests[0])
```


### Model Comparison

If correctly implements, the all three sum-to-zero constraints should properly identify the model in the same way.
Therefore we expect that all models produce the same estimates for the group-level parameters.

```{python}
# Use CmdStan's `stansummary` method to get summary statistics for all model parameters

ilr_fit_summary = binomial_ilr_fit.summary(sig_figs=2)
ilr_age_summary = ilr_fit_summary.filter(regex=r"\.*_age", axis=0)
ilr_eth_summary = ilr_fit_summary.filter(regex=r"\.*_eth", axis=0)
ilr_edu_summary = ilr_fit_summary.filter(regex=r"\.*_edu", axis=0)

hard_fit_summary = binomial_hard_fit.summary(sig_figs=2)
hard_age_summary = hard_fit_summary.filter(regex=r"\.*_age", axis=0)
hard_eth_summary = hard_fit_summary.filter(regex=r"\.*_eth", axis=0)
hard_edu_summary = hard_fit_summary.filter(regex=r"\.*_edu", axis=0)

soft_fit_summary = binomial_soft_fit.summary(sig_figs=2)
soft_age_summary = soft_fit_summary.filter(regex=r"\.*_age", axis=0)
soft_eth_summary = soft_fit_summary.filter(regex=r"\.*_eth", axis=0)
soft_edu_summary = soft_fit_summary.filter(regex=r"\.*_edu", axis=0)
```

**Global intercept**

* the hard-sum-to-zero model codes the global intercept as `beta_0`.
* the soft-sum-to-zero model 0-centers the binary predictor `sex`; `beta_intercept` accounts for this centering.

```{python}
print("global intercept", sim_data.beta_0[0])
ilr_fit_summary.loc[['beta_intercept', 'beta_0']]
```

```{python}
hard_fit_summary.loc[['beta_0']]
```

```{python}
soft_fit_summary.loc[['beta_intercept', 'beta_0']]
```

**Sex**

* the ilr model recodes the X matrix column `sex` as a zero-centered vector which is used to estimate `beta_sex`.
* the hard-sum-to-zero model codes `sex` as parameter `beta_sex_raw`, and in the transformed parameter block, defined `beta_sex[1]`, `beta_sex[2]`:

```stan
vector[2] beta_sex = [beta_sex_raw, -beta_sex_raw]';
```

```{python}
print("coefficient sex", sim_data.beta_sex[0])
print("per-category observation pcts hardcoded:  0.4, 0.6")
ilr_fit_summary.loc[['beta_sex']]
```

```{python}
hard_fit_summary.loc[['beta_sex_raw', 'beta_sex[1]', 'beta_sex[2]']]
```

```{python}
soft_fit_summary.loc[['beta_sex']]
```

**Age**

```{python}
print("true coeffecients age", sim_data.beta_age[0])
print("per-category observation pcts", sim_data.pct_age[0])
ilr_age_summary = ilr_fit_summary.filter(regex=r"\.*_age", axis=0)
ilr_age_summary
```

```{python}
hard_age_summary
```

```{python}
soft_age_summary
```

**Eth**

```{python}
print("true coeffecients eth", sim_data.beta_eth[0])
print("per-category observation pcts", sim_data.pct_eth[0])
ilr_eth_summary = ilr_fit_summary.filter(regex=r"\.*_eth", axis=0)
ilr_eth_summary
```

```{python}
hard_eth_summary
```

```{python}
soft_eth_summary
```

**Edu**

```{python}
print("true coeffecients edu", sim_data.beta_edu[0])
print("per-category observation pcts", sim_data.pct_edu[0])
ilr_edu_summary = ilr_fit_summary.filter(regex=r"\.*_edu", axis=0)
ilr_edu_summary
```

```{python}
hard_edu_summary
```

```{python}
soft_edu_summary
```

### Visualize the fit

Plot the observed data values against the predicted data values for a random sample of the replicates.

```{python}
obs_vs_rep_ilr_df = pd.DataFrame(data={'sim_data pos_tests': sim_data.pos_tests[0].astype(int)});
for x in range(0,100):
    draw = randint(0, 80);
    obs_vs_rep_ilr_df['iter ' + str(draw)] =  y_rep_ilr[draw]
# obs_vs_rep_ilr_df

obs_vs_rep_ilr_long = pd.melt(obs_vs_rep_ilr_df, id_vars=['sim_data pos_tests'], var_name='variable', value_name='value')

ilr_ppc = (p9.ggplot(obs_vs_rep_ilr_long, p9.aes(x='sim_data pos_tests', y='value')) +
    p9.geom_jitter(alpha=0.3, color='darkblue') +
    p9.geom_abline(color='orange') +
    p9.labs(x='pos_tests (simulated)', y='Posterior estiimates', title='Sum-to-zero vector\nPosterior Predictive Check') +
    p9.theme(figure_size=(10,10))
    )
ilr_ppc
```

Plot the distribution of the actual data against the predicted data values for a random sample of replicates.

```{python}
# plot the distribution of the actual data against a random sample of replicates.
yrep_ilr_pd = binomial_ilr_fit.draws_pd(vars='y_rep')

ppc_plot_ilr = ppc_density_plot(sim_df, yrep_ilr_pd, 160, 'PPC sum_to_zero_vector', 'sim data dark blue, y_rep sample light blue')
ppc_plot_ilr
```

## Spatial models with sum-to-zero constrained parameters

Spatial auto-correlation is the tendency for adjacent areas to share similar characteristics.
Conditional Auto-Regressive (CAR) and Intrinsic Conditional Auto-Regressive (ICAR) models,
first introduced by Besag (1974), account for this by pooling information from neighboring regions.
Specification of the global, or joint distribution via the local specification
of the conditional distributions of the individual random variables
defines a Gaussian Markov random field (GMRF) centered at $0$.

In this section we use a dataset of traffic accidents in New York City that
have been aggregated to the US Census tract level.
We use the US Census geodata map to determine the spatial structure of the data, then we fit a series of models,
starting from a baseline Poisson regression without any spatial component, then adding an ICAR component,
then the BYM2 model.   Finally, we consider the BYM2 model for maps with disconnected components and islands,
which is necessary given that New York City is comprised of several distinct land masses and a few islands.

### The Intrinsic Conditional Auto-Regressive (ICAR) Model

The ICAR model is widely used because the CAR model, like GPs, require computing matrix inverses.
In constrast, the ICAR model can handle maps containing thousands and tens of thousands of areal regions.

* Conditional specification: multivariate normal random vector $\mathbf{\phi}$
where each ${\phi}_i$ is conditional on the values of its neighbors

#### Joint specification of the ICAR variate $\phi$

* The joint specification of the ICAR rewrites to _Pairwise Difference_, centered at 0, assuming common variance for all elements of $\phi$.
$$ p(\phi) \propto \exp \left\{ {- \frac{1}{2}} \sum_{i \sim j}{({\phi}_i - {\phi}_j)}^2 \right\} $$

* Each ${({\phi}_i - {\phi}_j)}^2$ contributes a
penalty term based on the distance between the values of neighboring regions.
We use Stan's vectorized operations to compute log probability density:
```stan
   target += -0.5 * dot_self(phi[node1] - phi[node2]);
```

* $\phi$ is non-identifiable, constant added to $\phi$ washes out of ${\phi}_i - {\phi}_j$
  + sum-to-zero constraint centers $\phi$



### The BYM and BYM2 model

The ICAR model is computationally tractible, but fails to properly account for all regional variance.
The Besag York Mollié (BYM) model was developed to address this problem.
The BYM model uses both spatial ($\phi$) and non-spatial ($\theta$) error terms
to account for over-dispersion not modelled by the regression coefficients.
When the observed variance isn't fully explained by the spatial structure of the data,
an ordinary random effects component will account for the rest.
However, this model becomes difficult to fit
because either component can account for most or all of the individual-level variance.
Without any hyperpriors on $\phi$ and $\theta$ the sampler will be forced to explore
many extreme posterior probability distributions; the sampler will go very slowly or
fail to fit the data altogether.

The BYM2 model (Riebler et al, 2016) follows
the _Penalized Complexity_ framework (Simpson et al, 2017)
which favors models where the parameters have clear interpretations,
allowing for assignment of sensible hyperparameters to each.
Like the BYM model, the BYM2 model includes both spatial and non-spatial error terms
and like the alternative model of Leroux, Lei, and Breslow (Leroux et al, 2000)
it places a single precision (scale) parameter $\sigma$ on the combined components
and a mixing parameter $\rho$ for the amount of spatial/non-spatial variation.

$$\left( (\sqrt{\, {\rho} / s}\, \ )\,\phi^* + (\sqrt{1-\rho})\,\theta^* \right) \sigma $$

In order for $\sigma$ to legitimately be the standard deviation of the combined components,
it is critical that for each $i$, $\operatorname{Var}(\phi_i) \approx \operatorname{Var}(\theta_i) \approx 1$.
This is done by adding a scaling factor $s$ to the model which scales 
the proportion of variance $\rho$.
Because the scaling factor $s$ depends on the dataset, it comes into the model as data.


### Spatial Data Prep for ICAR, BYM2 models

The dataset we're using is that used in the analysis published in 2019
[Bayesian Hierarchical Spatial Models: Implementing the Besag York Mollié Model in Stan](https://www.sciencedirect.com/science/article/pii/S1877584518301175).

The data consists of motor vehicle collisions in New York City,
as recorded by the NYC Department of Transportation, between the years 2005-2014,
restricted to collisions involving school age children 5-18 years of age as pedestrians.
Each crash was localized to the US Census tract in which it occurred, using boundaries from the 2010 United States Census,
using the [2010 Census block map for New York City](https://data.cityofnewyork.us/City-Government/2010-Census-Blocks/v2h8-6mxf)

File `data/nyc_study.geojson` contains the study data and census tract ids and geometry.

#### Connected, disconnected components

In order to properly estimate an ICAR variate a set of areal regions, the neighborhood graph must be fully connected
so that all elements of the ICAR vector `phi` can be computed using the pairwise distance formula above.
Freni-Sterrantino et al.,2018 show how to extend the BYM2 model to account for maps with multiple components and islands - we will consider this model
at the end of this section.

#### Encoding the spatial structure of the data

* $N \times N$ Adjacency matrix - entries $(i,\ j)$ and $(j,\ i)$ are 1 when regions $n_i$ and $n_j$ are neighbors, 0 otherwise

* Undirected graph: regions are vertices, pairs of neighbors are edges, encoded as a 2 column matrix, each row is a pair of neighbors $({n_i}, {n_j})$

```stan
  int<lower = 0> N;  // number of areal regions
  // spatial structure
  int<lower = 0> N_edges;  // number of neighbor pairs
  array[2, N_edges] int<lower = 1, upper = N> neighbors;  // node[1, j] adjacent to node[2, j]
```

* Nodes are indexed from 1:N.
* Edges indices are stored in a 2 x N array
  + each column is an edge
  + row 1: index of first node in edge pair, $n_i$
  + row 2: index of second node in edge pair, $n_j$

```{python}
nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))
nyc_geodata.columns
```

```{python}
nyc_geodata[['BoroName', 'NTAName', 'count', 'kid_pop']].head(4)
```

```{python}
nyc_geodata[['BoroName', 'NTAName', 'count', 'kid_pop']].tail(4)
```

The shapefiles from the Census Bureau connect Manhattan to Brooklyn and Queens, but for this analysis, Manhattan is quite separate from Brooklyn and Queens.  Getting the data assembled in the order required for our analysis requires data munging, encapsulated in the Python functions in file `utils_nyc_map.py`.

The function `nyc_sort_by_comp_size` removes any neighbor pairs between tracts in Manhattan and any tracts in Brooklyn or Queens and updates the neighbor graph accordingly.  It returns a clean neighbor graph and the corresponding geodataframe, plus a list of the component sizes.   The list is sorted so that the largest component (Brooklyn and Queens) is first, and singleton nodes are last.

```{python}
(nyc_nbs, nyc_gdf, nyc_comp_sizes) = nyc_sort_by_comp_size(nyc_geodata)
nyc_comp_sizes
```

To check our work we examine both the geodataframe and the map.

```{python}
nyc_gdf[['BoroName', 'NTAName', 'count', 'kid_pop']].head(4)
```

```{python}
nyc_gdf[['BoroName', 'NTAName', 'count', 'kid_pop']].tail(4)
```

```{python}
from splot.libpysal import plot_spatial_weights 
plot_spatial_weights(nyc_nbs, nyc_gdf)
```

We restrict our attention to the largest connected component in order to evaluate the performace of different sum-to-zero constraints for the ICAR and BYM2 models.

```{python}
from libpysal.weights import Rook
brklyn_qns_gdf = nyc_gdf[nyc_gdf['comp_id']==0].reset_index(drop=True)
brklyn_qns_nbs = Rook.from_dataframe(brklyn_qns_gdf , geom_col='geometry')
plot_spatial_weights(brklyn_qns_nbs, brklyn_qns_gdf ) 

print(f'number of components: {brklyn_qns_nbs.n_components}')
print(f'islands? {brklyn_qns_nbs.islands}')
print(f'max number of neighbors per node: {brklyn_qns_nbs.max_neighbors}')
print(f'mean number of neighbors per node: {brklyn_qns_nbs.mean_neighbors}')
```

Create data dictionary of inputs to the ICAR and BYM2 models.

The data block for the BYM2 model is:

```stan
data {
  int<lower=0> N;
  array[N] int<lower=0> y; // count outcomes
  vector<lower=0>[N] E; // exposure
  int<lower=1> K; // num covariates
  matrix[N, K] xs; // design matrix

  // spatial structure
  int<lower = 0> N_edges;  // number of neighbor pairs
  array[2, N_edges] int<lower = 1, upper = N> neighbors;  // columnwise adjacent

  real tau; // scaling factor
}
```

The data block for the ICAR model is the same, minus the scaling factor `tau`.

* Compute `N_edges`, `neighbors` given neighbor graph.

```{python}
brklyn_qns_nbs_adj =  brklyn_qns_nbs.to_adjlist(remove_symmetric=True)
# create np.ndarray from columns in adjlist, increment indices by 1
j1 = brklyn_qns_nbs_adj['focal'] + 1
j2 = brklyn_qns_nbs_adj['neighbor'] + 1
edge_pairs = np.vstack([j1, j2])
edge_pairs.shape, edge_pairs
```

* Compute the scaling factor `tau`

```{python}
from utils_bym2 import get_scaling_factor
tau = get_scaling_factor(brklyn_qns_nbs)
tau
```

* All columns of the predictor matrix should be roughly the same scale - assemble the design matrix and scale columns.

```{python}
design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])

design_mat = brklyn_qns_gdf[design_vars].to_numpy()
design_mat[:, 1] = np.log(design_mat[:, 1])
design_mat[:, 2] = np.log(design_mat[:, 2])

pd.DataFrame(data=design_mat).describe()
```

* Assemble the data dict

```{python}
nyc_data_dict = {"N":brklyn_qns_gdf .shape[0],
             "y":brklyn_qns_gdf ['count'].astype('int'),
             "E":brklyn_qns_gdf ['kid_pop'].astype('int'),
             "K":design_mat.shape[1],
             "xs":design_mat,
             "N_edges": edge_pairs.shape[1],
             "neighbors": edge_pairs,
    	     "tau":tau
            }
```

### Baseline (non-spatial) model - Poisson regression

As a starting point, we consider a simple Poisson regression for this dataset which doesn't acount for spatial correlation between neighboring census tracts.

```{python}
poisson_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson.stan'))
poisson_fit = poisson_mod.sample(data=nyc_data_dict)

poisson_fit.summary().round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]
```

### Posterior Predictive Checks

In the `generated quantities` block, we create the [posterior predictive sample](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html), `y_rep`.    If the model is well-calibrated for the data, we expect that at least 50% of the time, the observed value of `y` will fall in the central 50% interval of the `y_rep` sample estimates.

```{python}
%run utils_dataviz.py
# Extract posterior predictive samples - shape (draws, N)
y_rep = poisson_fit.stan_variable("y_rep")

print(ppc_central_interval(y_rep, nyc_data_dict['y']))


ppc_plot = plot_post_pred_check(y_rep, nyc_data_dict['y'], 
                                'Poisson model, y (blue dot) vs. y_rep (orange 50% central interval, grep full extent)')
ppc_plot
```

The posterior predictive checks demonstrate that this data cannot be fit by a simple poisson regression.   Therefore we move on to the next models:  the ICAR and BYM2 models.

### ICAR evaluations

```{python}
icar_ilr_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_ilr.stan'))
icar_ilr_fit = icar_ilr_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'icar_ilr'))
```

```{python}
icar_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_soft.stan'))
icar_soft_fit = icar_soft_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'icar_soft'))
```

```{python}
icar_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_hard.stan'))
icar_hard_fit = icar_hard_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'icar_hard'))
```

Get summaries and compare fits (note - we can also view the outputs using the MCMC_monitor)

```{python}
icar_ilr_summary = icar_ilr_fit.summary()
icar_soft_summary = icar_soft_fit.summary()
icar_hard_summary = icar_hard_fit.summary()
```

```{python}
print("sum_to_zero_vector phi")
icar_ilr_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]
```

```{python}
print("soft sum to zero constrain phi")
icar_soft_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]
```

```{python}
print("hard sum to zero constrain phi")
icar_hard_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]
```

Of these models, `icar_ilr` which uses the built-in `sum_to_zero_vec` runs the fasted and has the highest effective sample size.   The soft sum-to-zero constraint runs almost as quickly, but has a lower effective sample size.  The hard sum-to-zero constraint runs 2-3 times slower - or more, depending on the initializations but has slightly better effective sample size than the soft sum-to-zero sample.  All models fit and produce the same estimates for the group-level parameters.

#### Posterior predictive checks

```{python}
y_rep = icar_ilr_fit.stan_variable("y_rep")

print(ppc_central_interval(y_rep, nyc_data_dict['y']))


ppc_plot = plot_post_pred_check(y_rep, nyc_data_dict['y'], 
                                'ICAR model, y (blue dot) vs. y_rep (orange 50% central interval, grey full extent)')
ppc_plot
```

The posterior predictive estimates are OK, but very noisy.

### The BYM2 model

```{python}
bym2_ilr_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_ilr.stan'))
bym2_ilr_fit = bym2_ilr_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'bym2_ilr'))
```

```{python}
bym2_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_soft.stan'))
bym2_soft_fit = bym2_soft_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'bym2_soft'))
```

```{python}
bym2_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_hard.stan'))
bym2_hard_fit = bym2_hard_mod.sample(data=nyc_data_dict, output_dir=os.path.join('mcmc_monitor', 'bym2_hard'))
```

```{python}
bym2_ilr_summary = bym2_ilr_fit.summary()
bym2_soft_summary = bym2_soft_fit.summary()
bym2_hard_summary = bym2_hard_fit.summary()
```

```{python}
print("sum_to_zero_vector phi")
bym2_ilr_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

```{python}
#| scrolled: true
print("soft sum to zero constrain phi")
bym2_soft_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

```{python}
print("hard sum to zero constrain phi")
bym2_hard_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

Of these models, the bym2_ilr which uses the built-in sum_to_zero_vec runs the fasted and has the highest effective sample size. The soft sum-to-zero constraint runs almost as quickly, but has a lower effective sample size. The hard sum-to-zero constraint runs 2-3 times slower - or more, depending on the initializations. All models fit and produce the same estimates for the group-level parameters.

#### Posterior predictive checks

```{python}
y_rep = bym2_ilr_fit.stan_variable("y_rep")
print(ppc_central_interval(y_rep, nyc_data_dict['y']))
ppc_plot = plot_post_pred_check(y_rep, nyc_data_dict['y'], 
                                'ICAR model, y (blue dot) vs. y_rep (orange 50% central interval, grey full extent)')
ppc_plot
```

### The BYM2 model for disconnected components and singletons

#### Assemble data

Data block of the Stan BYM2 model for disconnected components and islands:

```
  int<lower=0> N;
  array[N] int<lower=0> y; // count outcomes
  vector<lower=0>[N] E; // exposure
  int<lower=1> K; // num covariates
  matrix[N, K] xs; // design matrix

  // neighbor graph structure
  int<lower=0, upper=N> N_components;
  array[N_components] int<lower=1, upper=N> component_sizes;
  int<lower=0, upper=N> N_singletons;
  int<lower = 0> N_edges;  // number of neighbor pairs
  array[2, N_edges] int<lower = 1, upper = (N - N_singletons)> neighbors;  // columnwise adjacent
  vector<lower=0>[N_components] scaling_factors;
```

Get study data

```{python}
nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))
nyc_geodata.columns
```

```{python}
%run utils_nyc_map
nyc_nbs = Rook.from_dataframe(nyc_geodata, geom_col='geometry')
from splot.libpysal import plot_spatial_weights 
plot_spatial_weights(nyc_nbs, nyc_geodata)
```

The map shows that several tracts in Manhattan neighbor tracts in Brooklyn and Queens.
For the purposes of our analysis, these need to be edited out.

```{python}
(nyc_nbs, nyc_gdf, sizes) = nyc_sort_by_comp_size(nyc_geodata)
```

Check dataframe, neighbors graph.

```{python}
plot_spatial_weights(nyc_nbs, nyc_gdf)
```

```{python}
nyc_gdf[['NTAName', 'comp_id', 'comp_size']].head(4)
```

```{python}
nyc_gdf[['NTAName', 'comp_id', 'comp_size']].tail(4)
```

Assemble Poisson regression covariates.

```{python}
N = nyc_gdf.shape[0]
print("N", N)
y = nyc_gdf['count'].astype(int).tolist()
print("y", y[:7])
E = nyc_gdf['kid_pop'].astype(int).tolist()
print("E", E[:7])

x_cols = ['pct_pubtransit', 'med_hh_inc', 'traffic', 'frag_index']
K = len(x_cols)
print("K", K)
xs = nyc_gdf[x_cols].to_numpy()
### we need to standardize xs
xs[:, 1] = np.log(xs[:, 1])
xs[:, 2] = np.log(xs[:, 2])

print('xs', xs[:4, :])
pd.DataFrame(data=xs).describe()
```

```{python}
# compute number of components, sizes, and number of singleton nodes
print(sizes)
component_sizes = [x for x in sizes if x > 1]
N_components = len(component_sizes)
N_singletons = len(sizes) - N_components
print("N_components ", N_components, " N_singletons ", N_singletons, " component_sizes ", component_sizes)
```

```{python}
# compute neighbors array
nbs_adj =  nyc_nbs.to_adjlist(remove_symmetric=True)
print(nbs_adj.head(4))
print(nbs_adj.tail(4))
# create np.ndarray from columns in adjlist, increment indices by 1
j1 = nbs_adj['focal'] + 1
j2 = nbs_adj['neighbor'] + 1
neighbors = np.vstack([j1, j2])
N_edges = neighbors.shape[1]
print("N_edges", N_edges)
print("start array cols\n", neighbors[:, :10])
print("end array cols\n", neighbors[:, -10:]) 
```

```{python}
# get scaling factors
%run utils_bym2

scaling_factors = np.ones(N_components)
for i in range(N_components):
    comp_gdf = nyc_gdf[nyc_gdf['comp_id'] == i].reset_index(drop=True)
    comp_nbs = Rook.from_dataframe(comp_gdf, geom_col='geometry')
    # plot_spatial_weights(comp_nbs, comp_gdf)
    component_w = W(comp_nbs.neighbors, comp_nbs.weights)
    scaling_factors[i] = get_scaling_factor(component_w)

print(scaling_factors)
```

```{python}
# assemble nyc_data_dict
nyc_data_dict = {
    'N':N,
    'y':y,
    'E':E,
    'K':K,
    'xs':xs,
    'N_components':N_components,
    'component_sizes': component_sizes,
    'N_singletons':N_singletons,
    'N_edges':N_edges,
    'neighbors':neighbors,
    'scaling_factors': scaling_factors
}
```

```{python}
from cmdstanpy import write_stan_json
write_stan_json("nyc_bym2_multicomp.json", nyc_data_dict)
```

## Fit model

```{python}
bym2_multicomp_ilr = CmdStanModel(stan_file=os.path.join('stan', 'bym2_multicomp_ilr.stan'))
```

```{python}
bym2_multicomp_ilr_fit = bym2_multicomp_ilr.sample(data=nyc_data_dict)
```

```{python}
bym2_multicomp_soft = CmdStanModel(stan_file=os.path.join('stan', 'bym2_multicomp_soft.stan'))
```

```{python}
bym2_multicomp_soft_fit = bym2_multicomp_soft.sample(
    data=nyc_data_dict,
    max_treedepth=11)
```

```{python}
bym2_multicomp_ilr_summary = bym2_multicomp_ilr_fit.summary()
bym2_multicomp_soft_summary = bym2_multicomp_soft_fit.summary()
```

```{python}
print("sum_to_zero_vector phi")
bym2_multicomp_ilr_summary.round(2).loc[
  ['beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

```{python}
print("soft sum to zero constrain phi")
bym2_multicomp_soft_summary.round(2).loc[
  ['beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

## Posterior Predictive Checks

```{python}
%run utils_dataviz.py

y_rep_ilr = bym2_multicomp_ilr_fit.stan_variable("y_rep")
print(ppc_central_interval(y_rep_ilr, nyc_data_dict['y']))
ppc_plot_ilr = plot_post_pred_check(y_rep_ilr, nyc_data_dict['y'], 
                                'BYM2 multicomp sum_to_zero_vector\ny (blue dot) vs. y_rep (orange 50% central interval, grey full extent)')
ppc_plot_ilr
```

```{python}
phi_draws = bym2_multicomp_ilr_fit.stan_variable('phi')
phi_corr_ilr = upper_corr_matrix_to_df(phi_draws)
plot_corr_ilr = plot_icar_corr_matrix(phi_corr_ilr, "Spatial Correlation vector phi\nBYM2 multicomp sum_to_zero_vector", size=(16,16))
plot_corr_ilr
```

```{python}
comp_1_gdf = nyc_gdf[nyc_gdf['comp_id']==1].reset_index(drop=True)
phi_1_draws = bym2_multicomp_ilr_fit.stan_variable('phi_1')
phi_1_corr = upper_corr_matrix_to_df(phi_1_draws)
plot_corr_comp_1 = plot_icar_corr_matrix(phi_1_corr, "Spatial Correlation phi_1\nBYM2 multicomp sum_to_zero_vector", size=(12,12))
plot_corr_comp_1
```

```{python}
comp_2_gdf = nyc_gdf[nyc_gdf['comp_id']==2].reset_index(drop=True)
phi_2_draws = bym2_multicomp_ilr_fit.stan_variable('phi_2')
phi_2_corr = upper_corr_matrix_to_df(phi_2_draws)
plot_corr_comp_2 = plot_icar_corr_matrix(phi_2_corr, "Spatial Correlation phi_2\nBYM2 multicomp sum_to_zero_vector", size=(6,6))
plot_corr_comp_2
```

```{python}
comp_3_gdf = nyc_gdf[nyc_gdf['comp_id']==3].reset_index(drop=True)
phi_3_draws = bym2_multicomp_ilr_fit.stan_variable('phi_3')
phi_3_corr = upper_corr_matrix_to_df(phi_3_draws)
plot_corr_comp_3 = plot_icar_corr_matrix(phi_3_corr, "Spatial Correlation phi_3\nBYM2 multicomp sum_to_zero_vector", size=(6,6))
plot_corr_comp_3
```

```{python}
comp_4_gdf = nyc_gdf[nyc_gdf['comp_id']==4].reset_index(drop=True)
phi_4_draws = bym2_multicomp_ilr_fit.stan_variable('phi_4')
phi_4_corr = upper_corr_matrix_to_df(phi_4_draws)
plot_corr_comp_4 = plot_icar_corr_matrix(phi_4_corr, "Spatial Correlation phi_4\nBYM2 multicomp sum_to_zero_vector", size=(6,6))
plot_corr_comp_4
```

```{python}
comp_5_gdf = nyc_gdf[nyc_gdf['comp_id']==5].reset_index(drop=True)
phi_5_draws = bym2_multicomp_ilr_fit.stan_variable('phi_5')
phi_5_corr = upper_corr_matrix_to_df(phi_5_draws)
plot_corr_comp_5 = plot_icar_corr_matrix(phi_5_corr, "Spatial Correlation phi_5\nBYM2 multicomp sum_to_zero_vector", size=(6,6))
plot_corr_comp_5
```

Component 6 - Roosevelt Island - is a 2-node component.

```{python}
bym2_multicomp_ilr_summary.round(2).loc[['phi_6[1]', 'phi_6[2]']]
```

## References


* Riebler et al., 2016: [An intuitive Bayesian spatial model for disease mapping that accounts for scaling](https://arxiv.org/abs/1601.01180)

* Freni-Sterrantino et al.,2018: [A note on intrinsic conditional autoregressive models for disconnected graphs](https://arxiv.org/pdf/1705.04854.pdf)

* Morris et al., 2019: [Bayesian Hierarchical Spatial Models: Implementing the Besag York Mollié Model in Stan](https://www.sciencedirect.com/science/article/abs/pii/S1877584518301175)


