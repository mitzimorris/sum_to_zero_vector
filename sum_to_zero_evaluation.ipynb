{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sum-to-Zero Constraint in Stan\n",
    "\n",
    "## Introducing the `sum_to_zero_vector` Constrained Parameter Type\n",
    "\n",
    "As of Stan 2.36, there is a built in\n",
    "[`sum_to_zero_vector`](https://mc-stan.org/docs/reference-manual/transforms.html#zero-sum-vector)\n",
    "constrained parameter type.\n",
    "\n",
    "```stan\n",
    "parameters {\n",
    "  sum_to_zero_vector[K] beta;\n",
    "  // ...\n",
    "}\n",
    "```\n",
    "\n",
    "This produces a vector of size `K` such that `sum(beta) = 0`.  In the\n",
    "unconstrained representation requires only `K - 1` values because the\n",
    "last is determined by the first `K - 1`.\n",
    "\n",
    "Prior to Stan 2.36, a sum-to-zero constraint could be implemented in one of two ways:\n",
    "\n",
    "- using a \"hard\" sum to zero constraint, where the parameter is declared to be an $N-1$ length vector with a corresponding $N$-length transformed parameter\n",
    "whose first $N-1$ elements are the same as the corresponding parameter vector, and the $N^{th}$ element is the negative sum of the $N-1$ elements.\n",
    "- using a \"soft\" sum to zero constraint with an $N$-length parameter vector whose sum is constrained to be within $\\epsilon$ of $0$.\n",
    "\n",
    "The performance of these implementations depends on the size of the parameter vector:\n",
    "for small sizes, the hard sum-to-zero constraint is more efficient; for larger sizes, the soft sum-to-zero constraint is faster.\n",
    "\n",
    "In this notebook we show how the `sum_to_zero_vector` constraint provides consistently better performance\n",
    "than either of the above ways of imposing a sum to zero constraint on a parameter vector,\n",
    "by considering two different classes of models:\n",
    "\n",
    "- multi-level regressions for binomial data with group-level categorical predictors\n",
    "- spatial models for areal data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used in this notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotnine as p9\n",
    "import libpysal\n",
    "from splot.libpysal import plot_spatial_weights \n",
    "from random import randint\n",
    "\n",
    "from cmdstanpy import CmdStanModel, write_stan_json, cmdstan_path, cmdstan_version, rebuild_cmdstan\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook display options\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level models with group-level categorical predictors\n",
    "\n",
    "In this section we consider a model which estimates per-demographic\n",
    "disease prevalence rates for a population.\n",
    "\n",
    "The data consists of:\n",
    "\n",
    "* A set of per-demographic aggregated outcomes of a diagnostic test procedure\n",
    "with unequal number of tests per demographic.\n",
    "* A corresponding set of demographic descriptors encoded as a vector of categorical values.\n",
    "In this example these are named `sex`, `age`, `eth`, and `edu`, but there can be any number\n",
    "of categories.\n",
    "* The specified test sensitivity and specificity\n",
    "\n",
    "In order to fit this model, we need to put a sum-to-zero constraint on the categorical variables.\n",
    "\n",
    "\n",
    "### The data generating model\n",
    "\n",
    "We have written a data-generating program to create datasets given the\n",
    "baseline disease prevalence, test specificity and sensitivity,\n",
    "the specified total number of diagnostic tests,\n",
    "and number of categories for `age`, `eth`, and `edu`, `N_age`, `N_eth`, and `N_edu`, respectively.\n",
    "The total number of demographic strata is $2$ * `N_age` * `N_eth` * `N_edu`.\n",
    "\n",
    "```stan\n",
    "data {\n",
    "  int<lower=1> N;  // total number of tests\n",
    "  int<lower=1> N_age;\n",
    "  int<lower=1> N_eth;\n",
    "  int<lower=1> N_edu;\n",
    "  real baseline;\n",
    "  real<lower=0, upper=1> sens;\n",
    "  real<lower=0, upper=1> spec;\n",
    "}\n",
    "transformed data {\n",
    "  int strata = 2 * N_age * N_eth * N_edu;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "In the generated quantities block we first generate:\n",
    "\n",
    "* the true weights for the categorical coefficient vectors\n",
    "* the distribution of per-category observations\n",
    "\n",
    "```stan\n",
    "  real beta_0 = baseline;\n",
    "  // some difference by sex, unequal observations\n",
    "  real beta_sex = normal_rng(0, 0.5);\n",
    "  vector[2] pct_sex = [0.4, 0.6]';\n",
    "\n",
    "  vector[N_age] pct_age = dirichlet_rng(rep_vector(2, N_age));\n",
    "  vector[N_age] beta_age;\n",
    "  for (n in 1:N_age) {\n",
    "    beta_age[n] = std_normal_rng();\n",
    "  }\n",
    "  ...\n",
    "```\n",
    "\n",
    "Then we use a set of nested loops to generate the table of positive tests, total tests per category.\n",
    "\n",
    "```stan\n",
    "  array[strata] int sex;\n",
    "  array[strata] int age;\n",
    "  array[strata] int eth;\n",
    "  array[strata] int edu;\n",
    "  array[strata] int pos_tests;\n",
    "  array[strata] int tests;\n",
    "  array[strata] real p;\n",
    "  array[strata] real p_sample;\n",
    "\n",
    "  int idx = 1;\n",
    "  for (i_sex in 1:2) {\n",
    "    for (i_age in 1:N_age) {\n",
    "      for (i_eth in 1:N_eth) {\n",
    "        for (i_edu in 1:N_edu) {\n",
    "          sex[idx] = i_sex;\n",
    "          age[idx] = i_age;\n",
    "          eth[idx] = i_eth;\n",
    "          edu[idx] = i_edu;\n",
    "          tests[idx] = to_int(pct_sex[i_sex] * pct_age[i_age] * pct_eth[i_eth] * pct_edu[i_edu] * N);\n",
    "          p[idx] = inv_logit(beta_0 + beta_sex * (i_sex)\n",
    "                    + beta_age[i_age] + beta_eth[i_eth] +  beta_edu[i_edu]);\n",
    "          p_sample[idx] = p[idx] * sens + (1 - p[idx]) * (1 - spec);\n",
    "          pos_tests[idx] = binomial_rng(tests[idx], p_sample[idx]);\n",
    "          idx += 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "### Creating Simulated Datasets\n",
    "\n",
    "This program allows us to generate datasets for large and small populations\n",
    "and for finer or more coarse-grained sets of categories.\n",
    "The larger the number of strata overall, the more observations are needed to get good coverage.\n",
    "\n",
    "* Instantiate the data generating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mod = CmdStanModel(stan_file=os.path.join('stan', \n",
    "                                              'gen_binomial_4_preds.stan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the number of categories for age, eth, and edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dict = {\n",
    "    'N_age':10,\n",
    "    'N_eth':3,\n",
    "    'N_edu':8,\n",
    "    'baseline': -3.9,\n",
    "    'sens': 0.75,\n",
    "    'spec': 0.9995}\n",
    "\n",
    "print(\"total strata\",\n",
    "      (2 * gen_data_dict['N_age'] * gen_data_dict['N_eth'] * gen_data_dict['N_edu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the total number of observations.\n",
    "Here we generate two datasets:  one with a small number of observations, relative to the number of strata,\n",
    "and one with sufficient data to provide information on all combinations of demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = gen_data_dict.copy()\n",
    "gen_data['N'] = 9600\n",
    "\n",
    "gen_data_large = gen_data_dict.copy()\n",
    "gen_data_large['N'] = 960000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run 1 sampling iteration to get a complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = gen_mod.sample(data=gen_data,\n",
    "                          iter_warmup=1, iter_sampling=1, chains=1, seed=45678)\n",
    "\n",
    "sim_data_large = gen_mod.sample(data=gen_data_large,\n",
    "                          iter_warmup=1, iter_sampling=1, chains=1, seed=45678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine the set of generated data-generating params and resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N = 9600 strata = 480, ratio 20:1\\n\")\n",
    "for var, value in sim_data.stan_variables().items():\n",
    "    print(var, value[0]) if isinstance(value[0], np.float64) else print(var, value[0][:10])\n",
    "\n",
    "\n",
    "print(\"\\n\\nN = 960000 strata = 480, ratio 2000:1\\n\")\n",
    "for var, value in sim_data_large.stan_variables().items():\n",
    "    print(var, value[0]) if isinstance(value[0], np.float64) else print(var, value[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the distribution of observed positive tests and the underlying prevalence.\n",
    "\n",
    "Because the data-generating parameters and percentage of observations per category are generated at random,\n",
    "some datasets may have very low overall disease rates and/or many unobserved strata, and will therefore be\n",
    "pathologically hard to fit.  This is informative for understanding what is consistent when\n",
    "generating a set of percentages and regression weights as is done in the Stan data generating program.\n",
    "\n",
    "```stan\n",
    "  vector[N_eth] pct_eth = dirichlet_rng(rep_vector(1, N_eth));\n",
    "  for (n in 1:N_eth) {\n",
    "    beta_eth[n] = std_normal_rng();\n",
    "  }\n",
    "```\n",
    "\n",
    "However, this can result in very unbalanced datasets, in which case it is best to\n",
    "generate another dataset and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame({'tests':sim_data.tests[0], 'pos_tests':sim_data.pos_tests[0], 'p_sample':sim_data.p_sample[0] })\n",
    "sim_df['raw_prev'] = sim_df['pos_tests'] / sim_df['tests']\n",
    "(\n",
    "    p9.ggplot(sim_df)\n",
    "    + p9.geom_density(p9.aes(x='raw_prev'), color='darkblue', fill='blue', alpha=0.3)\n",
    "    + p9.geom_density(p9.aes(x='p_sample'), color='darkred', fill='pink', alpha=0.5)\n",
    "    + p9.labs(\n",
    "        x='raw prevalence',\n",
    "        y='',\n",
    "        title='Observed (orange) and underlying true prevalence (blue)\\nsmall dataset'\n",
    "    )\n",
    "    + p9.theme_minimal()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame({'tests':sim_data_large.tests[0], 'pos_tests':sim_data_large.pos_tests[0], 'p_sample':sim_data_large.p_sample[0] })\n",
    "sim_df['raw_prev'] = sim_df['pos_tests'] / sim_df['tests']\n",
    "(\n",
    "    p9.ggplot(sim_df)\n",
    "    + p9.geom_density(p9.aes(x='raw_prev'), color='darkblue', fill='blue', alpha=0.3)\n",
    "    + p9.geom_density(p9.aes(x='p_sample'), color='darkred', fill='pink', alpha=0.5)\n",
    "    + p9.labs(\n",
    "        x='raw prevalence',\n",
    "        y='',\n",
    "        title='Observed (orange) and underlying true prevalence (blue)\\nlarge dataset'\n",
    "    )\n",
    "    + p9.theme_minimal()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Assemble the data dictionary of all input data for the model which solves the inverse problem -\n",
    "i.e., estimates regression coefficients given the observed data.\n",
    "We use the generated data as the inputs.\n",
    "Because the output files are real-valued outputs, regardless of variable element type,\n",
    "model data variables of type int need to be cast to int.\n",
    "Here all the observed data is count and categorial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4_preds = {'N':sim_data.pos_tests.shape[1], \n",
    "                'N_age':gen_data_dict['N_age'], \n",
    "                'N_eth':gen_data_dict['N_eth'],\n",
    "                'N_edu':gen_data_dict['N_edu'],\n",
    "                'pos_tests':sim_data.pos_tests[0].astype(int),\n",
    "                'tests':sim_data.tests[0].astype(int),\n",
    "                'sex':sim_data.sex[0].astype(int),\n",
    "                'age':sim_data.age[0].astype(int), \n",
    "                'eth':sim_data.eth[0].astype(int),\n",
    "                'edu':sim_data.edu[0].astype(int),\n",
    "                'sens': gen_data_dict['sens'],\n",
    "                'spec': gen_data_dict['spec'],\n",
    "                'intercept_prior_mean': gen_data_dict['baseline'],\n",
    "                'intercept_prior_scale': 2.5}\n",
    "\n",
    "data_4_preds_large = {'N':sim_data_large.pos_tests.shape[1], \n",
    "                'N_age':gen_data_dict['N_age'], \n",
    "                'N_eth':gen_data_dict['N_eth'],\n",
    "                'N_edu':gen_data_dict['N_edu'],\n",
    "                'pos_tests':sim_data_large.pos_tests[0].astype(int),\n",
    "                'tests':sim_data_large.tests[0].astype(int),\n",
    "                'sex':sim_data_large.sex[0].astype(int),\n",
    "                'age':sim_data_large.age[0].astype(int), \n",
    "                'eth':sim_data_large.eth[0].astype(int),\n",
    "                'edu':sim_data_large.edu[0].astype(int),\n",
    "                'sens': gen_data_dict['sens'],\n",
    "                'spec': gen_data_dict['spec'],\n",
    "                'intercept_prior_mean': gen_data_dict['baseline'],\n",
    "                'intercept_prior_scale': 2.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Capture the data-generating params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_params = {\n",
    "    'beta_0': sim_data.beta_0[0],\n",
    "    'pct_sex': sim_data.pct_sex[0],\n",
    "    'beta_sex': sim_data.beta_sex[0],\n",
    "    'pct_age': sim_data.pct_age[0],\n",
    "    'beta_age':sim_data.beta_age[0],\n",
    "    'pct_eth': sim_data.pct_eth[0],\n",
    "    'beta_eth':sim_data.beta_eth[0],\n",
    "    'pct_edu': sim_data.pct_edu[0],\n",
    "    'beta_edu':sim_data.beta_edu[0]\n",
    "}\n",
    "true_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: `sum_to_zero_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_ozs_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_ozs.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_ozs_fit = binomial_ozs_mod.sample(data=data_4_preds, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_ozs_fit_large = binomial_ozs_mod.sample(data=data_4_preds_large, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2:  Hard sum-to-zero constraint\n",
    "\n",
    "Run the sampler to get posterior estimates of the model conditioned on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_hard.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_hard_fit = binomial_hard_mod.sample(data=data_4_preds, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_hard_fit_large = binomial_hard_mod.sample(data=data_4_preds_large, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3:  soft sum-to-zero constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'binomial_4preds_soft.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_soft_fit = binomial_soft_mod.sample(data=data_4_preds, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial_soft_fit_large = binomial_soft_mod.sample(data=data_4_preds_large, parallel_chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime performance\n",
    "\n",
    "In the small data regime, the soft-sum to zero takes considerably more wall-clock time to fit the data.\n",
    "On Apple M3 hardware, all three models quickly fit the large dataset.\n",
    "\n",
    "\n",
    "### Model Checking and Comparison\n",
    "\n",
    "Run CmdStan's `diagnose` method to check model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum_to_zero_vector\\n\")\n",
    "print(binomial_ozs_fit.diagnose())\n",
    "print(\"\\n\\nhard sum-to-zero\\n\")\n",
    "print(binomial_hard_fit.diagnose())\n",
    "print(\"\\n\\nsoft sum-to-zero\\n\")\n",
    "print(binomial_soft_fit.diagnose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calibration check**\n",
    "All models contain a `generated quantities` block, which creates `y_rep`,\n",
    "the [posterior predictive sample](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html).\n",
    "If the model is well-calibrated for the data, \n",
    "we expect that at least 50% of the time the observed value of `y` will fall in the central 50% interval of the `y_rep` sample estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_dataviz import ppc_central_interval\n",
    "\n",
    "y_rep_ozs = binomial_ozs_fit.y_rep.astype(int)\n",
    "print(\"sum_to_zero_vector fit\", ppc_central_interval(y_rep_ozs, sim_data.pos_tests[0]))\n",
    "\n",
    "y_rep_hard = binomial_hard_fit.y_rep.astype(int)\n",
    "print(\"Hard sum-to-zero fit\", ppc_central_interval(y_rep_hard, sim_data.pos_tests[0]))\n",
    "\n",
    "y_rep_soft = binomial_soft_fit.y_rep.astype(int)\n",
    "print(\"Soft sum-to-zero fit\", ppc_central_interval(y_rep_soft, sim_data.pos_tests[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three sum-to-zero constraints should properly identify the model in the same way\n",
    "and therefore all models should produce the same estimates for the group-level parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CmdStan's `stansummary` method to get summary statistics for all model parameters\n",
    "ozs_fit_summary = binomial_ozs_fit.summary(sig_figs=2)\n",
    "hard_fit_summary = binomial_hard_fit.summary(sig_figs=2)\n",
    "soft_fit_summary = binomial_soft_fit.summary(sig_figs=2)\n",
    "\n",
    "ozs_age_summary = ozs_fit_summary.filter(regex=r\"\\.*_age\", axis=0)\n",
    "ozs_eth_summary = ozs_fit_summary.filter(regex=r\"\\.*_eth\", axis=0)\n",
    "ozs_edu_summary = ozs_fit_summary.filter(regex=r\"\\.*_edu\", axis=0)\n",
    "\n",
    "hard_age_summary = hard_fit_summary.filter(regex=r\"\\.*_age\", axis=0)\n",
    "hard_eth_summary = hard_fit_summary.filter(regex=r\"\\.*_eth\", axis=0)\n",
    "hard_edu_summary = hard_fit_summary.filter(regex=r\"\\.*_edu\", axis=0)\n",
    "\n",
    "soft_age_summary = soft_fit_summary.filter(regex=r\"\\.*_age\", axis=0)\n",
    "soft_eth_summary = soft_fit_summary.filter(regex=r\"\\.*_eth\", axis=0)\n",
    "soft_edu_summary = soft_fit_summary.filter(regex=r\"\\.*_edu\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global intercept**\n",
    "\n",
    "* the hard-sum-to-zero model codes the global intercept as `beta_0`.\n",
    "* the soft-sum-to-zero model 0-centers the binary predictor `sex`; `beta_intercept` accounts for this centering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"global intercept\", sim_data.beta_0[0])\n",
    "ozs_fit_summary.loc[['beta_intercept', 'beta_0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_fit_summary.loc[['beta_0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_fit_summary.loc[['beta_intercept', 'beta_0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sex**\n",
    "\n",
    "* the ozs model recodes the X matrix column `sex` as a zero-centered vector which is used to estimate `beta_sex`.\n",
    "* the hard-sum-to-zero model codes `sex` as parameter `beta_sex_raw`, and in the transformed parameter block, defined `beta_sex[1]`, `beta_sex[2]`:\n",
    "\n",
    "```stan\n",
    "vector[2] beta_sex = [beta_sex_raw, -beta_sex_raw]';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coefficient sex\", sim_data.beta_sex[0])\n",
    "print(\"per-category observation pcts hardcoded:  0.4, 0.6\")\n",
    "ozs_fit_summary.loc[['beta_sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_fit_summary.loc[['beta_sex_raw', 'beta_sex[1]', 'beta_sex[2]']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_fit_summary.loc[['beta_sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true coeffecients age\", sim_data.beta_age[0])\n",
    "print(\"per-category observation pcts\", sim_data.pct_age[0])\n",
    "ozs_age_summary = ozs_fit_summary.filter(regex=r\"\\.*_age\", axis=0)\n",
    "ozs_age_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_age_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_age_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true coeffecients eth\", sim_data.beta_eth[0])\n",
    "print(\"per-category observation pcts\", sim_data.pct_eth[0])\n",
    "ozs_eth_summary = ozs_fit_summary.filter(regex=r\"\\.*_eth\", axis=0)\n",
    "ozs_eth_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_eth_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_eth_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true coeffecients edu\", sim_data.beta_edu[0])\n",
    "print(\"per-category observation pcts\", sim_data.pct_edu[0])\n",
    "ozs_edu_summary = ozs_fit_summary.filter(regex=r\"\\.*_edu\", axis=0)\n",
    "ozs_edu_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_edu_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_edu_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the fit\n",
    "\n",
    "Plot the distribution of the actual data against the predicted data values for a random sample of replicates.\n",
    "In file `utils_dataviz.py` we have implemented the equivalent of the R `bayesplot` package function\n",
    "`ppc_dens_overlay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_dataviz import ppc_dens_overlay\n",
    "yrep_ozs = binomial_ozs_fit.stan_variable('y_rep')\n",
    "ppc_plot_ozs = ppc_dens_overlay(sim_data.pos_tests[0].astype('int'), yrep_ozs, 100, 'PPC sum_to_zero_vector', 'counts')\n",
    "ppc_plot_ozs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "While all implementations of the sum-to-zero constraint fit the model,\n",
    "the use of `sum_to_zero_vector` provides the fastest wall-clock time\n",
    "and the best effective sample size results.\n",
    "\n",
    "\n",
    "## Spatial models with sum-to-zero constrained parameters\n",
    "\n",
    "Spatial auto-correlation is the tendency for adjacent areas to share similar characteristics.\n",
    "Conditional Auto-Regressive (CAR) and Intrinsic Conditional Auto-Regressive (ICAR) models,\n",
    "first introduced by Besag (1974), account for this by pooling information from neighboring regions.\n",
    "Specification of the global, or joint distribution via the local specification\n",
    "of the conditional distributions of the individual random variables\n",
    "defines a Gaussian Markov random field (GMRF) centered at $0$.\n",
    "Zero-centering implies a sum-to-zero constraint.\n",
    "As before, we consider the three different ways to implement this constraint in Stan.\n",
    "\n",
    "In this section we use a dataset of traffic accidents in New York City\n",
    "aggregated to the US Census tract level consisting of\n",
    "counts of events per census tract, child population per census tract,\n",
    "and per-tract predictors.\n",
    "The baseline model used to estimate the rate of events per tract is a Poisson regression.\n",
    "```stan\n",
    "  y ~ poisson_log(log_E + beta0 + xs * betas);\n",
    "```\n",
    "Because the observed number of events (traffic accidents) per census tract is low,\n",
    "the Poisson regression fails to account for the overdispersion present in the data.\n",
    "\n",
    "To address this problem, we first add a simple ICAR component to the Poisson regression,\n",
    "using the US Census geodata map to determine the spatial structure of the data.\n",
    "Then we consider the BYM2 model of Riebler et. al., a refinement of\n",
    "the  Besag York Mollié (BYM) model, which accounts for\n",
    "both spatial and heterogenous variation across regions.\n",
    "Finally, we consider the BYM2 model for maps with disconnected components and islands,\n",
    "which is necessary in order to properly analyze the NYC dataset,\n",
    "since New York City is comprised of several distinct land masses and a few islands.\n",
    "\n",
    "#### The Intrinsic Conditional Auto-Regressive (ICAR) Model\n",
    "\n",
    "The ICAR model is widely used because the CAR model, like GPs, require computing matrix inverses.\n",
    "In constrast, the ICAR model can handle maps containing thousands and tens of thousands of areal regions.\n",
    "\n",
    "* Conditional specification: multivariate normal random vector $\\mathbf{\\phi}$\n",
    "where each ${\\phi}_i$ is conditional on the values of its neighbors\n",
    "\n",
    "* The joint specification of the ICAR rewrites to _Pairwise Difference_, centered at 0, assuming common variance for all elements of $\\phi$.\n",
    "$$ p(\\phi) \\propto \\exp \\left\\{ {- \\frac{1}{2}} \\sum_{i \\sim j}{({\\phi}_i - {\\phi}_j)}^2 \\right\\} $$\n",
    "\n",
    "* Each ${({\\phi}_i - {\\phi}_j)}^2$ contributes a\n",
    "penalty term based on the distance between the values of neighboring regions.\n",
    "We use Stan's vectorized operations to compute log probability density:\n",
    "```stan\n",
    "   target += -0.5 * dot_self(phi[node1] - phi[node2]);\n",
    "```\n",
    "\n",
    "* $\\phi$ is non-identifiable, constant added to $\\phi$ washes out of ${\\phi}_i - {\\phi}_j$\n",
    "  + sum-to-zero constraint centers $\\phi$\n",
    "\n",
    "#### The BYM and BYM2 model\n",
    "\n",
    "The Besag York Mollié (BYM) model \n",
    "uses both spatial ($\\phi$) and non-spatial ($\\theta$) error terms\n",
    "to account for over-dispersion not modelled by the regression coefficients.\n",
    "When the observed variance isn't fully explained by the spatial structure of the data,\n",
    "an ordinary random effects component will account for the rest.\n",
    "However, this model becomes difficult to fit\n",
    "because either component can account for most or all of the individual-level variance.\n",
    "Without any hyperpriors on $\\phi$ and $\\theta$ the sampler will be forced to explore\n",
    "many extreme posterior probability distributions; the sampler will go very slowly or\n",
    "fail to fit the data altogether.\n",
    "\n",
    "The BYM2 model (Riebler et al, 2016) follows\n",
    "the _Penalized Complexity_ framework (Simpson et al, 2017)\n",
    "which favors models where the parameters have clear interpretations,\n",
    "allowing for assignment of sensible hyperparameters to each.\n",
    "Like the BYM model, the BYM2 model includes both spatial and non-spatial error terms\n",
    "and like the alternative model of Leroux, Lei, and Breslow (Leroux et al, 2000)\n",
    "it places a single precision (scale) parameter $\\sigma$ on the combined components\n",
    "and a mixing parameter $\\rho$ for the amount of spatial/non-spatial variation.\n",
    "\n",
    "$$\\left( (\\sqrt{\\, {\\rho} / s}\\, \\ )\\,\\phi^* + (\\sqrt{1-\\rho})\\,\\theta^* \\right) \\sigma $$\n",
    "\n",
    "In order for $\\sigma$ to legitimately be the standard deviation of the combined components,\n",
    "it is critical that for each $i$, $\\operatorname{Var}(\\phi_i) \\approx \\operatorname{Var}(\\theta_i) \\approx 1$.\n",
    "This is done by adding a scaling factor $s$ to the model which scales \n",
    "the proportion of variance $\\rho$.\n",
    "Because the scaling factor $s$ depends on the dataset, it comes into the model as data.\n",
    "\n",
    "#### Graph Connectivity and Modeling Consequences\n",
    "\n",
    "Neighbor graphs are used to model spatial relationships between areal regions\n",
    "(such as states, districts, or census tracts). Nodes in the graph correspond to regions\n",
    "and edges connect nodes whose corresponding regions share a common point or boundary line.\n",
    "Edges have no direction, i.e., an edge between $i$ and $j$ implies\n",
    "that $i$ is a neighbor of $j$ and $j$ is a neighbor of $i$.\n",
    "\n",
    "*Components* represent groups of regions that are connected, either by a direct edge or a series of edges.\n",
    "The size of a component is the number of nodes in it.\n",
    "An island, or singleton, is a component of size 1, so we must distinguish between *connected components*\n",
    "of size 2 or greater and *singletons*.\n",
    "\n",
    "If it is possible to find a path (series of edges) which connects any region in the graph to any\n",
    "other region, then the graph is *fully connected*, i.e., it consists of a single\n",
    "component.  For example, at the state level, the neighbor graph of the 48 continental states of the\n",
    "U.S. is fully connected, but the graph of all 50 U.S. states and territories is *disconnected*;\n",
    "it consists of a connected component and several islands.\n",
    "\n",
    "The ICAR model requires that the neighbor graph is fully connected for two reasons:\n",
    "\n",
    "* The joint distribution is computed from the pairwise differences between a node and its neighbors;\n",
    "singleton nodes have no neighbors and are therefore undefined.\n",
    "\n",
    "* Even if the graph doesn't have any singleton nodes, when the graph has multiple connected components\n",
    "a sum-to-zero constraint on the entire vector fails to properly identify the model.\n",
    "\n",
    "#### Encoding the spatial structure of the data\n",
    "\n",
    "An undirected neighbor graph can be encoded either as a matrix of as a list of edgepairs.\n",
    "\n",
    "* $N \\times N$ Adjacency matrix - entries $(i,\\ j)$ and $(j,\\ i)$ are 1 when regions $n_i$ and $n_j$ are neighbors, 0 otherwise\n",
    "\n",
    "* Edgepairs: regions are given a sequential id number.\n",
    "The graph is encoded as a 2 row matrix, where each column is a pair of neighbor ids $({n_i}, {n_j})$\n",
    "\n",
    "```stan\n",
    "  int<lower = 0> N;  // number of areal regions\n",
    "  // spatial structure\n",
    "  int<lower = 0> N_edges;  // number of neighbor pairs\n",
    "  array[2, N_edges] int<lower = 1, upper = N> neighbors;  // node[1, j] adjacent to node[2, j]\n",
    "```\n",
    "\n",
    "* Nodes are indexed from 1:N.\n",
    "* Edges indices are stored in a 2 x N array\n",
    "  + each column is an edge\n",
    "  + row 1: index of first node in edge pair, $n_i$\n",
    "  + row 2: index of second node in edge pair, $n_j$\n",
    "\n",
    "#### From GIS shapefiles to neighbor graphs\n",
    "\n",
    "Areal data consists of a finite set of regions with well-defined boundaries.\n",
    "Geographic information systems (GIS) encode areal boundaries as a set of bounding polygons.\n",
    "To represent other features, e.g. area name and demographic together with GIS data\n",
    "we use packages which extend tabular dataframes to GIS data\n",
    "\n",
    "- Python: [`GeoPandas`](https://geopandas.org/en/stable/index.html) - add support for geographic data to pandas objects. \n",
    "- R: [`sf`](https://r-spatial.github.io/sf/) Simple Features for R - \"spatial analysis simplified\"\n",
    "\n",
    "By defining a neighbor relationship between area regions, we take the information from a GIS dataframe\n",
    "and convert it to a graph, where the areas are the nodes and edges denote the neighbor relationship.\n",
    "To compute neighbor graphs we use the following packages\n",
    "\n",
    "- Python: [`libpysal`](https://pysal.org/libpysal/) the Python Spatial Analysis Library Core\n",
    "- R: [`spdep`](https://r-spatial.r-universe.dev/spdep) Spatial Dependence: Weighting Schemes, Statistics\n",
    "\n",
    "There are many ways to define contiguity, of these two straightforward metrics\n",
    "are *Rook* and *Queen* - regions which share bounding line are neighbors,\n",
    "and regions which share a bounding line or vertex are neighbors, respectively.\n",
    "\n",
    "\n",
    "### Example dataset:  New York City traffic accidents\n",
    "\n",
    "The dataset we're using is that used in the analysis published in 2019\n",
    "[Bayesian Hierarchical Spatial Models: Implementing the Besag York Mollié Model in Stan](https://www.sciencedirect.com/science/article/pii/S1877584518301175).\n",
    "\n",
    "The data consists of motor vehicle collisions in New York City,\n",
    "as recorded by the NYC Department of Transportation, between the years 2005-2014,\n",
    "restricted to collisions involving school age children 5-18 years of age as pedestrians.\n",
    "Each crash was localized to the US Census tract in which it occurred, using boundaries from the 2010 United States Census,\n",
    "using the [2010 Census block map for New York City](https://data.cityofnewyork.us/City-Government/2010-Census-Blocks/v2h8-6mxf).  File `data/nyc_study.geojson` contains the study data and census tract ids and geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))\n",
    "nyc_geodata.columns\n",
    "nyc_geodata[['BoroName', 'NTAName', 'count', 'kid_pop']].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapefiles from the Census Bureau connect Manhattan to Brooklyn and Queens, but for this analysis, Manhattan is quite separate from Brooklyn and Queens.  Getting the data assembled in the order required for our analysis requires data munging, encapsulated in the Python functions in file `utils_nyc_map.py`.\n",
    "The function `nyc_sort_by_comp_size` removes any neighbor pairs between tracts in Manhattan and any tracts in Brooklyn or Queens and updates the neighbor graph accordingly.  It returns a clean neighbor graph and the corresponding geodataframe, plus a list of the component sizes.   The list is sorted so that the largest component (Brooklyn and Queens) is first, and singleton nodes are last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nyc_map import nyc_sort_by_comp_size\n",
    "\n",
    "(nyc_nbs, nyc_gdf, nyc_comp_sizes) = nyc_sort_by_comp_size(nyc_geodata)\n",
    "nyc_comp_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our work we examine both the geodataframe and the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_gdf[['BoroName', 'NTAName', 'count', 'kid_pop']].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_gdf[['BoroName', 'NTAName', 'count', 'kid_pop']].tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splot.libpysal import plot_spatial_weights \n",
    "plot_spatial_weights(nyc_nbs, nyc_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both the ICAR and BYM2 model require a fully connected neighbor graph,\n",
    "we restrict our attention to the largest graph component, which is comprised\n",
    "of Brooklyn and Queens (excepting the Rockaways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import Queen\n",
    "brklyn_qns_gdf = nyc_gdf[nyc_gdf['comp_id']==0].reset_index(drop=True)\n",
    "brklyn_qns_nbs = Queen.from_dataframe(brklyn_qns_gdf , geom_col='geometry')\n",
    "plot_spatial_weights(brklyn_qns_nbs, brklyn_qns_gdf ) \n",
    "\n",
    "print(f'number of components: {brklyn_qns_nbs.n_components}')\n",
    "print(f'islands? {brklyn_qns_nbs.islands}')\n",
    "print(f'max number of neighbors per node: {brklyn_qns_nbs.max_neighbors}')\n",
    "print(f'mean number of neighbors per node: {brklyn_qns_nbs.mean_neighbors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model one:  the ICAR model\n",
    "\n",
    "* Besag, 1973, 1974:  Conditional Auto-Regressive Model (CAR), and\n",
    "**Intrinsic Conditional Auto-Regressive** (ICAR) model\n",
    "\n",
    "* Conditional specification: multivariate normal random vector $\\mathbf{\\phi}$\n",
    "where each ${\\phi}_i$ is conditional on the values of its neighbors\n",
    "\n",
    "* Joint specification rewrites to _Pairwise Difference_,\n",
    "$$ p(\\phi) \\propto \\exp \\left\\{ {- \\frac{1}{2}} \\sum_{i \\sim j}{({\\phi}_i - {\\phi}_j)}^2 \\right\\} $$\n",
    "\n",
    "* Each ${({\\phi}_i - {\\phi}_j)}^2$ contributes a\n",
    "penalty term based on the distance between the values of neighboring regions\n",
    "\n",
    "* $\\phi$ is non-identifiable, constant added to $\\phi$ washes out of ${\\phi}_i - {\\phi}_j$\n",
    "    + A sum-to-zero constraint identifies and centers $\\phi$\n",
    "\n",
    "We consider the three possible Stan implementations of this constraint.\n",
    "\n",
    "#### Stan models\n",
    "\n",
    "The `stan` folder contains implementations the Poisson + ICAR model,\n",
    "which differ only in their implementation of the sum-to-zero constraint.\n",
    "Below is the implementation which uses the `sum_to_zero_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_model_file = os.path.join('stan', 'poisson_icar_ozs.stan')\n",
    "\n",
    "with open(icar_model_file, 'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for ICAR and BYM2 models.\n",
    "from utils_bym2 import get_scaling_factor, nbs_to_adjlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brklyn_qns_nbs_adj = nbs_to_adjlist(brklyn_qns_nbs)\n",
    "brklyn_qns_nbs_adj\n",
    "\n",
    "design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])\n",
    "\n",
    "design_mat = brklyn_qns_gdf[design_vars].to_numpy()\n",
    "design_mat[:, 1] = np.log(design_mat[:, 1])\n",
    "design_mat[:, 2] = np.log(design_mat[:, 2])\n",
    "\n",
    "pd.DataFrame(data=design_mat).describe()\n",
    "\n",
    "nyc_data = {\"N\":brklyn_qns_gdf.shape[0],\n",
    "            \"y\":brklyn_qns_gdf['count'].astype('int'),\n",
    "            \"E\":brklyn_qns_gdf['kid_pop'].astype('int'),\n",
    "            \"K\":4,\n",
    "            \"xs\":design_mat,\n",
    "            \"N_edges\": brklyn_qns_nbs_adj.shape[1],\n",
    "            \"neighbors\": brklyn_qns_nbs_adj\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_ozs_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_ozs.stan'))\n",
    "icar_ozs_fit = icar_ozs_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_soft.stan'))\n",
    "icar_soft_fit = icar_soft_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'poisson_icar_hard.stan'))\n",
    "icar_hard_fit = icar_hard_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "\n",
    "Get summaries and compare fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_ozs_summary = icar_ozs_fit.summary()\n",
    "icar_soft_summary = icar_soft_fit.summary()\n",
    "icar_hard_summary = icar_hard_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum_to_zero_vector phi\")\n",
    "icar_ozs_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"soft sum to zero constrain phi\")\n",
    "icar_soft_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hard sum to zero constrain phi\")\n",
    "icar_hard_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Of these models, `icar_ozs` which uses the built-in `sum_to_zero_vector` has the best\n",
    "wall clock time and the highest effective sample size.\n",
    "The soft sum-to-zero constraint runs almost as quickly, but has a lower effective sample size.\n",
    "The hard sum-to-zero constraint runs 2-3 times slower - or more, depending on the initializations,\n",
    "but has slightly better effective sample size than the soft sum-to-zero sample.\n",
    "All models fit and produce the same estimates for the group-level parameters.\n",
    "\n",
    "\n",
    "\n",
    "### Model Two: The BYM2 Model, Riebler et al, 2016\n",
    "\n",
    "The ICAR model assumes complete spatial correlation between regions.\n",
    "This simplifying assumption makes it cheap to compute;  the CAR model,\n",
    "which adds a weight to each neighbor pair to account for the strength of\n",
    "the spatial correlation, requires computing the inverse of the\n",
    "adjacency matrix, an cubic operation.  For maps with more than a handful of regions,\n",
    "this is prohibitively expensive, computationally.   The Besag York Mollié (BYM)\n",
    "model was developed to address this problem; the BYM2 model builds on this model\n",
    "and subsequent refinements.\n",
    "\n",
    "* Poisson regression + component for extra variation which\n",
    "combines spatial and non-spatial components $\\phi + \\theta$ as\n",
    "$$\\left( (\\sqrt{\\, {\\rho} / s}\\, \\ )\\,\\phi^* + (\\sqrt{1-\\rho})\\,\\theta^* \\right) \\sigma $$\n",
    "where:\n",
    "  + $\\sigma\\geq 0$ is the overall standard deviation.\n",
    "  + $\\rho \\in [0,1]$ - proportion of spatial variance.\n",
    "  + $\\phi^*$ is the ICAR component.\n",
    "  + $\\theta^* \\sim N(0, 1)$ is the vector of ordinary random effects\n",
    "  + $s$ is a scaling factor s.t. $\\operatorname{Var}(\\phi_i) \\approx 1$; $s$ is data.\n",
    "\n",
    "* Follows previous BYM modifications, notably Leroux 2000,\n",
    "which has a single scale parameter plus mixing parameter.\n",
    "\n",
    "* Adds a scaling factor on the ICAR component so that for each element $i$ in vectors $\\theta$ and $\\phi$,\n",
    "$\\operatorname{Var}(\\theta_i) \\approx \\operatorname{Var}(\\phi_i) \\approx 1$.\n",
    "The scaling factor is derived from the adjacency matrix (neighbors graph).\n",
    "The prior on `phi` is the ICAR component, the spatial covariance matrix.\n",
    "Riebler et. al. recommend scaling it by the geometric mean of\n",
    "the variance of the spatial adjacency matrix.\n",
    "\n",
    "The Stan implementation of the BYM2 model is an expansion of the Poisson ICAR model.\n",
    "\n",
    "```stan\n",
    "data {\n",
    "  // ... same as for Poisson ICAR\n",
    "  real tau; // scaling factor\n",
    "}\n",
    "transformed data {\n",
    "  // ... unchanged - log_E + center predictors\n",
    "}\n",
    "parameters {\n",
    "  real beta0; // intercept\n",
    "  vector[K] betas; // covariates\n",
    "  real<lower=0, upper=1> rho; // proportion of spatial variance\n",
    "  sum_to_zero_vector[N] phi;  // spatial effects\n",
    "  vector[N] theta; // heterogeneous random effects\n",
    "  real<lower = 0> sigma;  // scale of combined effects\n",
    "}\n",
    "transformed parameters {\n",
    "  vector[N] gamma = sqrt(1 - rho) * theta + sqrt(rho * inv(tau)) * phi;  // BYM2\n",
    "}\n",
    "model {\n",
    "  y ~ poisson_log(log_E + beta0 + xs_centered * betas + gamma * sigma);\n",
    "  target += -0.5 * dot_self(phi[neighbors[1]] - phi[neighbors[2]]); // ICAR prior\n",
    "  rho ~ beta(0.5, 0.5);\n",
    "  theta ~ std_normal();\n",
    "  // .. plus priors beta0, betas, sigma\n",
    "}\n",
    "```\n",
    "\n",
    "#### Data assembly\n",
    "\n",
    "The inputs to the BYM2 model are the same as for the ICAR model, with the addition of\n",
    "the scaling factor, which comes in as data.\n",
    "We have written a helper function called `get_scaling_factor`, in file `utils_bym2.py`\n",
    "which takes as its argument the neighbor graph and computes the geometric mean of the\n",
    "corresponding adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = get_scaling_factor(brklyn_qns_nbs)\n",
    "nyc_data['tau'] = tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_ozs_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_ozs.stan'))\n",
    "bym2_ozs_fit = bym2_ozs_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_soft_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_soft.stan'))\n",
    "bym2_soft_fit = bym2_soft_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_hard_mod = CmdStanModel(stan_file=os.path.join('stan', 'bym2_hard.stan'))\n",
    "bym2_hard_fit = bym2_hard_mod.sample(data=nyc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "\n",
    "Get summaries and compare fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_ozs_summary = bym2_ozs_fit.summary()\n",
    "bym2_soft_summary = bym2_soft_fit.summary()\n",
    "bym2_hard_summary = bym2_hard_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum_to_zero_vector phi\")\n",
    "bym2_ozs_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"soft sum to zero constrain phi\")\n",
    "bym2_soft_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hard sum to zero constrain phi\")\n",
    "bym2_hard_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: the `sum_to_zero_vector` just works!\n",
    "\n",
    "For the multi-level regression model, the ICAR model and the BYM2 model,\n",
    "the built-in `sum_to_zero_vector` returns the best effective sample size and\n",
    "has the best wall-clock time as well.  This is most evident for the BYM2 model,\n",
    "which has a relatively complex multilevel structure.\n",
    "For the the multi-level regression model, the soft-sum-to-zero constraint struggles\n",
    "when there is not much data.  Conversely, the hard-sum-to-zero constraint\n",
    "struggles to fit the NYC dataset, where the sum-to-zero constraint is applied to\n",
    "a relatively large vector.\n",
    "\n",
    "\n",
    "## Implementing the `sum_to_zero_vector` as a Stan program function\n",
    "\n",
    "In this section we consider the BYM2 model for maps with disconnected components and islands,\n",
    "which is necessary in order to properly analyze the NYC dataset,\n",
    "since New York City is comprised of several distinct land masses and a few islands.\n",
    "\n",
    "### BYM2_mutlicomp Model\n",
    "\n",
    "In order to fit the entire NYC dataset, \n",
    "we extend the BYM2 model to account for disconnected graphs and islands,\n",
    "following the recommendations from\n",
    "[A note on intrinsic Conditional Autoregressive models for disconnected graphs](https://arxiv.org/abs/1705.04854),\n",
    "Freni-Sterrantino et.al. 2018. \n",
    "\n",
    "* Component nodes are given the BYM2 prior\n",
    "* Singleton nodes (islands) are given a standard Normal prior\n",
    "* Compute per-connected component scaling factor\n",
    "* Impose a sum-to-zero constraint on each connected component\n",
    "\n",
    "In the BYM2 model for a fully connected graph the sum-to-zero constraint on `phi`\n",
    "is implemented directly by declaring `phi` to be a `sum_to_zero_vector`, which is a\n",
    "[constrained parameter type](https://mc-stan.org/docs/reference-manual/transforms.html#variable-transforms.chapter).\n",
    "The declaration:\n",
    "\n",
    "```stan\n",
    "  sum_to_zero_vector[N] phi;  // spatial effects\n",
    "```\n",
    "\n",
    "creates a *constrained* variable of length $N$, with a corresponding unconstrained variable of length $N-1$.\n",
    "\n",
    "For the BYM2_multicomp model, we need to do declare the *unconstrained* parameter vector `phi_raw`\n",
    "and the constraining transform, which is applied component-wise, to slices of `phi_raw`.\n",
    "The number of connected components is $N\\_components$, therefore the length of vector `phi_raw` is\n",
    "$N - N\\_components$.\n",
    "\n",
    "```stan\n",
    "  vector[N - N_components] phi_raw;  // spatial effects\n",
    "```\n",
    "\n",
    "In the `functions` block, we define two functions\n",
    "\n",
    "* `zero_sum_constrain_lp`: the constraining transform, following the `zero_sum_vector` implementation.\n",
    "* `zero_sum_components_lp`: slices vector `phi` by component, applies constraining transform to each.\n",
    "\n",
    "\n",
    "The constrained variable `phi` is defined in the `transformed parameters` block:\n",
    "\n",
    "```stan\n",
    "  vector[N_connected] phi = zero_sum_components_lp(phi_raw, component_idxs, component_sizes);\n",
    "```\n",
    "\n",
    "The function `zero_sum_components_lp` handles the (tedious) indexing needed to map `phi_raw`\n",
    "into `phi` - it is specific to this use case.\n",
    "The key function is `zero_sum_constrain_lp`\n",
    "\n",
    "\n",
    "```stan\n",
    "  /**\n",
    "   * Constrain sum-to-zero vector\n",
    "   *\n",
    "   * @param y unconstrained zero-sum parameters\n",
    "   * @return vector z, the vector whose slices sum to zero\n",
    "   */\n",
    "  vector zero_sum_constrain_lp(vector y) {\n",
    "    int N = num_elements(y);\n",
    "    vector[N + 1] z = zeros_vector(N + 1);\n",
    "    real sum_w = 0;\n",
    "    for (ii in 1:N) {\n",
    "      int i = N - ii + 1; \n",
    "      real n = i;\n",
    "      real w = y[i] * inv_sqrt(n * (n + 1));\n",
    "      sum_w += w;\n",
    "      z[i] += sum_w;     \n",
    "      z[i + 1] -= w * n;    \n",
    "    }\n",
    "    return z;\n",
    "  }\n",
    "```\n",
    "\n",
    "#### Putting it all together:  `bym2_multicomp.stan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_multicomp_file = os.path.join('stan', 'bym2_multicomp.stan')\n",
    "\n",
    "with open(bym2_multicomp_file, 'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data assembly\n",
    "\n",
    "We use the entire NYC dataset.\n",
    "In addition to the data inputs for the BYM2 model,\n",
    "we need to include the number of connected components (i.e., not singletons),\n",
    "and instead of a single scaling factor `tau`, a vector of per-component scaling factors `taus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = nyc_gdf.shape[0]\n",
    "y = nyc_gdf['count'].astype('int')\n",
    "E = nyc_gdf['kid_pop'].astype('int')\n",
    "K = 4\n",
    "\n",
    "design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])\n",
    "design_mat = nyc_gdf[design_vars].to_numpy()\n",
    "design_mat[:, 1] = np.log(design_mat[:, 1])\n",
    "design_mat[:, 2] = np.log(design_mat[:, 2])\n",
    "\n",
    "nyc_nbs_adj = nbs_to_adjlist(nyc_nbs)\n",
    "\n",
    "component_sizes = [x for x in nyc_comp_sizes if x > 1]\n",
    "N_components = len(component_sizes)\n",
    "scaling_factors = np.ones(N_components)\n",
    "for i in range(N_components):\n",
    "    comp_gdf = nyc_gdf[nyc_gdf['comp_id'] == i].reset_index(drop=True)\n",
    "    comp_nbs = libpysal.weights.Queen.from_dataframe(comp_gdf, geom_col='geometry')\n",
    "    component_w = libpysal.weights.W(comp_nbs.neighbors, comp_nbs.weights)\n",
    "    scaling_factors[i] = get_scaling_factor(component_w)\n",
    "\n",
    "print(scaling_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_multicomp_data = {\n",
    "    'N':N,\n",
    "    'y':y,\n",
    "    'E':E,\n",
    "    'K':K,\n",
    "    'xs':design_mat,\n",
    "    'N_components':N_components,\n",
    "    'component_sizes': component_sizes,\n",
    "    'N_edges':nyc_nbs_adj.shape[1],\n",
    "    'neighbors':nyc_nbs_adj,\n",
    "    'scaling_factors': scaling_factors\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_multicomp_mod = CmdStanModel(stan_file=bym2_multicomp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_multicomp_fit = bym2_multicomp_mod.sample(data=bym2_multicomp_data, iter_warmup=3000, iter_sampling=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_multicomp_summary = bym2_multicomp_fit.summary()\n",
    "bym2_multicomp_summary.round(2).loc[\n",
    "  ['beta_intercept', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full writeup of the BYM2_multicomp model is available in [this notebook](https://github.com/mitzimorris/geomed_2024/blob/main/h6_bym2_multicomp.ipynb).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we have demonstrated that the `sum_to_zero_vector`\n",
    "performs well in different models, with different vector sizes and data regimes.\n",
    "When a sum-to-zero constraint is required for slices of a vector,\n",
    "this same transform can be implemented as a Stan function.\n",
    "\n",
    "The `sum_to_zero_vector` really just works because the underlying transform -\n",
    "the first part of an isometric log ration transform\n",
    "results in equal variances of the the constrained sum to zero vector.\n",
    "As models increase in complexity, this becomes increasingly important.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
